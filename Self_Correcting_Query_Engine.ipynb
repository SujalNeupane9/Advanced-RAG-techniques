{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2XGO4UmwiWqF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install -q --upgrade pypdf llama-index llama-index-llms-gemini llama-index-embeddings-gemini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get the data\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4tTX5Qsi1Us",
        "outputId": "8be1f842-42a0-4ec6-bd86-fa88201a34d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-29 15:13:16--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  56.1MB/s    in 0.3s    \n",
            "\n",
            "2024-06-29 15:13:16 (56.1 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "P7S3tyM8i1SA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "OvgNPS9Ti1Pg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Load the data\n",
        "documents = SimpleDirectoryReader(input_files=['metagpt.pdf']).load_data()"
      ],
      "metadata": {
        "id": "orWuEbNBi1MG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_embedding_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
        "llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "K3jAWCJojK6Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = gemini_embedding_model"
      ],
      "metadata": {
        "id": "9YQM0eptjK2O"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex(documents)"
      ],
      "metadata": {
        "id": "ARMSxuXkjKvB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do agents share information with other agents?\"\n",
        "\n",
        "base_query_engine = index.as_query_engine()\n",
        "response = base_query_engine.query(query)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ItsrHvzbjKnZ",
        "outputId": "9d7ee5c0-b72a-4cdc-be5c-b661d56d10b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents share and comply with development and communication protocols defined in MetaGPT. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retry Query Engine"
      ],
      "metadata": {
        "id": "Taaln1_7jdkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetryQueryEngine\n",
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "\n",
        "query_response_evaluator = RelevancyEvaluator()\n",
        "retry_query_engine = RetryQueryEngine(\n",
        "    base_query_engine, query_response_evaluator\n",
        ")\n",
        "retry_response = retry_query_engine.query(query)\n",
        "print(retry_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mdar02UYjKgS",
        "outputId": "8f7f85f7-04af-470b-cd2e-a2f2c7e3f6d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents share and comply with development and communication protocols defined in MetaGPT. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RetrySourceQueryEngine\n",
        "\n",
        "retry_source_query_engine = RetrySourceQueryEngine(\n",
        "    base_query_engine, query_response_evaluator\n",
        ")\n",
        "retry_source_response = retry_source_query_engine.query(query)\n",
        "print(retry_source_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VgANapfpjl0h",
        "outputId": "0bf66fc9-97dc-4cdc-92e5-2aff17e5d070"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents share and comply with development and communication protocols defined in MetaGPT. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import GuidelineEvaluator\n",
        "from llama_index.core.evaluation.guideline import DEFAULT_GUIDELINES\n",
        "from llama_index.core import Response\n",
        "from llama_index.core.indices.query.query_transform.feedback_transform import (\n",
        "    FeedbackQueryTransformation,\n",
        ")\n",
        "from llama_index.core.query_engine import RetryGuidelineQueryEngine\n",
        "\n",
        "# Guideline eval\n",
        "guideline_eval = GuidelineEvaluator(\n",
        "    guidelines=DEFAULT_GUIDELINES\n",
        "    + \"\\nThe response should not be overly long.\\n\"\n",
        "    \"The response should try to summarize where possible.\\n\"\n",
        ")  # just for example"
      ],
      "metadata": {
        "id": "si-hcRCdjpu7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##under the hood\n",
        "typed_response = (\n",
        "    response if isinstance(response, Response) else response.get_response()\n",
        ")\n",
        "eval = guideline_eval.evaluate_response(query, typed_response)\n",
        "print(f\"Guideline eval evaluation result: {eval.feedback}\")\n",
        "\n",
        "feedback_query_transform = FeedbackQueryTransformation(resynthesize_query=True)\n",
        "transformed_query = feedback_query_transform.run(query, {\"evaluation\": eval})\n",
        "print(f\"Transformed query: {transformed_query.query_str}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pupAvy0xjwbJ",
        "outputId": "62339bb9-4c90-45e2-b3a1-7954d840370c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guideline eval evaluation result: The response is vague and does not fully answer the query. It mentions MetaGPT but does not explain how agents share information within it. The response lacks specifics and does not provide any statistics or numbers. It is also overly short and does not summarize the information.\n",
            "Transformed query: Here is a previous bad answer.\n",
            "Agents share and comply with development and communication protocols defined in MetaGPT. \n",
            "\n",
            "Here is some feedback from the evaluator about the response given.\n",
            "The response is vague and does not fully answer the query. It mentions MetaGPT but does not explain how agents share information within it. The response lacks specifics and does not provide any statistics or numbers. It is also overly short and does not summarize the information.\n",
            "Now answer the question.\n",
            "Please provide more context about the agents you are referring to.  For example:\n",
            "\n",
            "* **What kind of agents are these?** Are they software agents, AI agents, human agents, or something else?\n",
            "* **What is the context of their interaction?** Are they part of a specific system or platform? \n",
            "* **What kind of information are they sharing?** Is it data, instructions, knowledge, or something else?\n",
            "\n",
            "Once I have a better understanding of the context, I can provide a more helpful and informative response. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retry_guideline_query_engine = RetryGuidelineQueryEngine(\n",
        "    base_query_engine, guideline_eval, resynthesize_query=True\n",
        ")\n",
        "retry_guideline_response = retry_guideline_query_engine.query(query)\n",
        "print(retry_guideline_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "6a5Y1hTzjzjp",
        "outputId": "5de256b2-921d-40f6-ef2c-9c7442a94817"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents in MetaGPT communicate through a shared message pool, where they publish structured messages that other agents can subscribe to based on their profiles. This allows agents to obtain directional information from other roles and public information from the environment. For example, an Architect might publish a message containing the system design components, such as File Lists, Data Structures, and Interface Definitions. Engineers, who are subscribed to messages related to system design, can then access this information to execute the designated classes and functions. Similarly, a Product Manager might publish a message containing user requirements, which the Architect can then use to translate into system design components. This structured communication protocol ensures efficient information sharing and collaboration among agents, enabling them to work together effectively to complete complex tasks. \n",
            "\n"
          ]
        }
      ]
    }
  ]
}