{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3wC90f6_LjXB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install -q --upgrade pypdf llama-index llama-index-llms-gemini llama-index-embeddings-gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeiC7dD343CZ",
        "outputId": "abe08178-7a48-4095-f0a0-362de0c10907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-29 06:59:30--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘metagpt.pdf’\n",
            "\n",
            "metagpt.pdf         100%[===================>]  16.13M  52.1MB/s    in 0.3s    \n",
            "\n",
            "2024-06-29 06:59:31 (52.1 MB/s) - ‘metagpt.pdf’ saved [16911937/16911937]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## get the data\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "vFwBPKjxxaeO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "d1By4_Fb5Ioo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nest_asyncio\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "from llama_index.core.selectors import LLMMultiSelector\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sFNPA2hk6Kh9"
      },
      "outputs": [],
      "source": [
        "##Load the data\n",
        "documents = SimpleDirectoryReader(input_files=['metagpt.pdf']).load_data()\n",
        "\n",
        "##split the document\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sfifzI8DnOzV"
      },
      "outputs": [],
      "source": [
        "# initialize storage context (by default it's in-memory)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_embedding_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
        "llm = Gemini(model_name=\"models/gemini-1.5-flash-latest\")"
      ],
      "metadata": {
        "id": "nPKp07tvxWbd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Global settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = gemini_embedding_model"
      ],
      "metadata": {
        "id": "3qlPbVy_yFLq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fQW9tVuiekzV"
      },
      "outputs": [],
      "source": [
        "# llm  = LlamaCPP(\n",
        "#    model_url='https://huggingface.co/bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF/resolve/main/Llama-3-8B-Instruct-Gradient-1048k-Q5_K_S.gguf',\n",
        "#    model_path=None,\n",
        "#    temperature=0.1,\n",
        "#    max_new_tokens=256,\n",
        "#    context_window=3900,\n",
        "#    generate_kwargs={},\n",
        "#    #model_kwargs={\"n_gpu_layers\":-1},\n",
        "#    verbose=True\n",
        "# )\n",
        "# Settings.llm = llm\n",
        "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ekL4wPqj6KYo"
      },
      "outputs": [],
      "source": [
        "##define summary and vector index on the same data\n",
        "summary_index = SummaryIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "##Define Query Engines and Set Metadata\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description = (\n",
        "        \"Useful for Summarization questions related to MetaGPT\"\n",
        "    )\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the MetaGPT paper\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "KREiI4lf8mro",
        "outputId": "28f070e3-9b5b-400e-df83-1c3856e01dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which aligns with the purpose of summarization..\n",
            "\u001b[0mThis paper introduces MetaGPT, a meta-programming framework that leverages SOPs to enhance the problem-solving capabilities of multi-agent systems based on Large Language Models (LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simulated towns and the Minecraft Sandbox in V oyager. MetaGPT leverages role specialization, workflow management, and efficient sharing mechanisms such as message pools and subscriptions, rendering it a flexible and portable platform for autonomous agents and multi-agent frameworks. It uses an executable feedback mechanism to enhance code generation quality during runtime. In extensive experiments, MetaGPT achieves state-of-the-art performance on multiple benchmarks. The successful integration of human-like SOPs inspires future research on human-inspired techniques for artificial multi-agent systems. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMMultiSelector.from_defaults(),\n",
        "    #prompt_template=\"{query_str}\\n\\nPlease provide the answer in the following JSON format:\\n\",\n",
        "    query_engine_tools=[summary_tool,vector_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "t5PwWHUs9Ed1",
        "outputId": "89056572-b1c7-463a-8802-36f74ad46011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is relevant because it focuses on retrieving specific context from the MetaGPT paper, which is likely to contain information about how agents share information..\n",
            "\u001b[0mThe provided context does not contain information about how agents share information with other agents. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How do agents share information with other agents?\")\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}