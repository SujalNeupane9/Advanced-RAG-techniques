{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n! pip install -q --upgrade llama-index llama-index llama-index-llms-llama-cpp llama-index-readers-file pymupdf llama-index-embeddings-huggingface","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:41:40.759977Z","iopub.execute_input":"2024-06-24T08:41:40.760362Z","iopub.status.idle":"2024-06-24T08:43:51.529156Z","shell.execute_reply.started":"2024-06-24T08:41:40.760328Z","shell.execute_reply":"2024-06-24T08:43:51.527885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:44:12.165393Z","iopub.execute_input":"2024-06-24T08:44:12.165797Z","iopub.status.idle":"2024-06-24T08:44:12.194354Z","shell.execute_reply.started":"2024-06-24T08:44:12.165764Z","shell.execute_reply":"2024-06-24T08:44:12.193164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Settings\nfrom llama_index.llms.llama_cpp import LlamaCPP","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T08:45:04.088454Z","iopub.execute_input":"2024-06-24T08:45:04.088842Z","iopub.status.idle":"2024-06-24T08:45:04.248666Z","shell.execute_reply.started":"2024-06-24T08:45:04.088812Z","shell.execute_reply":"2024-06-24T08:45:04.246952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the sentence window node parser w/ default settings\nnode_parser = SentenceWindowNodeParser.from_defaults(\n    window_size=3,\n    window_metadata_key=\"window\",\n    original_text_metadata_key=\"original_text\",\n)\n\n# base node parser is a sentence splitter\ntext_splitter = SentenceSplitter()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:44:45.797276Z","iopub.execute_input":"2024-06-24T08:44:45.798306Z","iopub.status.idle":"2024-06-24T08:44:46.575028Z","shell.execute_reply.started":"2024-06-24T08:44:45.798270Z","shell.execute_reply":"2024-06-24T08:44:46.573634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the retriever model\nlama2 = LlamaCPP(\n   model_url='https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf',\n   model_path=None,\n   temperature=0.1,\n   max_new_tokens=256,\n   context_window=3900,\n   generate_kwargs={},\n   model_kwargs={\"n_gpu_layers\":-1},\n   verbose=True\n)\n\nSettings.llm = lama2\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nSettings.text_splitter = text_splitter","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:45:07.774769Z","iopub.execute_input":"2024-06-24T08:45:07.775488Z","iopub.status.idle":"2024-06-24T08:48:27.646778Z","shell.execute_reply.started":"2024-06-24T08:45:07.775454Z","shell.execute_reply":"2024-06-24T08:48:27.645302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data, Build the Index","metadata":{}},{"cell_type":"code","source":"!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:48:33.820383Z","iopub.execute_input":"2024-06-24T08:48:33.820797Z","iopub.status.idle":"2024-06-24T08:48:35.857396Z","shell.execute_reply.started":"2024-06-24T08:48:33.820765Z","shell.execute_reply":"2024-06-24T08:48:35.855835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core import SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\n    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n).load_data()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:48:39.793575Z","iopub.execute_input":"2024-06-24T08:48:39.794020Z","iopub.status.idle":"2024-06-24T08:49:38.555198Z","shell.execute_reply.started":"2024-06-24T08:48:39.793979Z","shell.execute_reply":"2024-06-24T08:49:38.554231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes = node_parser.get_nodes_from_documents(documents)\nbase_nodes = text_splitter.get_nodes_from_documents(documents)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:50:33.676994Z","iopub.execute_input":"2024-06-24T08:50:33.677795Z","iopub.status.idle":"2024-06-24T08:50:38.226277Z","shell.execute_reply.started":"2024-06-24T08:50:33.677760Z","shell.execute_reply":"2024-06-24T08:50:38.225177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##build the index\nfrom llama_index.core import VectorStoreIndex\n\nsentence_index = VectorStoreIndex(nodes)\nbase_index = VectorStoreIndex(base_nodes)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T08:51:24.501339Z","iopub.execute_input":"2024-06-24T08:51:24.501761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n\nquery_engine = sentence_index.as_query_engine(\n    similarity_top_k=2,\n    # the target key defaults to `window` to match the node_parser's default\n    node_postprocessors=[\n        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    ],\n)\nwindow_response = query_engine.query(\n    \"What are the concerns surrounding the AMOC?\"\n)\nprint(window_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window = window_response.source_nodes[0].node.metadata[\"window\"]\nsentence = window_response.source_nodes[0].node.metadata[\"original_text\"]\n\nprint(f\"Window: {window}\")\nprint(\"------------------\")\nprint(f\"Original Sentence: {sentence}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contrast with normal VectorStoreIndex","metadata":{}},{"cell_type":"code","source":"query_engine = base_index.as_query_engine(similarity_top_k=2)\nvector_response = query_engine.query(\n    \"What are the concerns surrounding the AMOC?\"\n)\nprint(vector_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_engine = base_index.as_query_engine(similarity_top_k=3)\nvector_response = query_engine.query(\n    \"What are the concerns surrounding the AMOC?\"\n)\nprint(vector_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"code","source":"for source_node in window_response.source_nodes:\n    print(source_node.node.metadata[\"original_text\"])\n    print(\"--------\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for node in vector_response.source_nodes:\n    print(\"AMOC mentioned?\", \"AMOC\" in node.node.text)\n    print(\"--------\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vector_response.source_nodes[2].node.text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n\nfrom llama_index.llms.openai import OpenAI\nimport nest_asyncio\nimport random\n\nnest_asyncio.apply()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama3 =  LlamaCPP(\n   model_url='https://huggingface.co/bartowski/Llama-3-8B-Instruct-Gradient-1048k-GGUF/resolve/main/Llama-3-8B-Instruct-Gradient-1048k-Q5_K_S.gguf',\n   model_path=None,\n   temperature=0.1,\n   max_new_tokens=256,\n   context_window=3900,\n   generate_kwargs={},\n   model_kwargs={\"n_gpu_layers\":-1},\n   verbose=True\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_nodes_eval = 30\n# there are 428 nodes total. Take the first 200 to generate questions (the back half of the doc is all references)\nsample_eval_nodes = random.sample(base_nodes[:200], num_nodes_eval)\n# NOTE: run this if the dataset isn't already saved\n# generate questions from the largest chunks (1024)\ndataset_generator = DatasetGenerator(\n    sample_eval_nodes,\n    llm=llama3,\n    show_progress=True,\n    num_questions_per_chunk=2,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_dataset.save_json(\"data/ipcc_eval_qr_dataset.json\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optional\neval_dataset = QueryResponseDataset.from_json(\"data/ipcc_eval_qr_dataset.json\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core.evaluation import (\n    CorrectnessEvaluator,\n    SemanticSimilarityEvaluator,\n    RelevancyEvaluator,\n    FaithfulnessEvaluator,\n    PairwiseComparisonEvaluator,\n)\n\n\nfrom collections import defaultdict\nimport pandas as pd\n\n# NOTE: can uncomment other evaluators\nevaluator_c = CorrectnessEvaluator(llm=llama3)\nevaluator_s = SemanticSimilarityEvaluator()\nevaluator_r = RelevancyEvaluator(llm=llama3)\nevaluator_f = FaithfulnessEvaluator(llm=llama3)\npairwise_evaluator = PairwiseComparisonEvaluator(llm=llama3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core.evaluation.eval_utils import (\n    get_responses,\n    get_results_df,\n)\nfrom llama_index.core.evaluation import BatchEvalRunner\n\nmax_samples = 10\n\neval_qs = eval_dataset.questions\nref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n\n# resetup base query engine and sentence window query engine\n# base query engine\nbase_query_engine = base_index.as_query_engine(similarity_top_k=2)\n# sentence window query engine\nquery_engine = sentence_index.as_query_engine(\n    similarity_top_k=2,\n    # the target key defaults to `window` to match the node_parser's default\n    node_postprocessors=[\n        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n    ],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nbase_pred_responses = get_responses(\n    eval_qs[:max_samples], base_query_engine, show_progress=True\n)\npred_responses = get_responses(\n    eval_qs[:max_samples], query_engine, show_progress=True\n)\n\npred_response_strs = [str(p) for p in pred_responses]\nbase_pred_response_strs = [str(p) for p in base_pred_responses]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator_dict = {\n    \"correctness\": evaluator_c,\n    \"faithfulness\": evaluator_f,\n    \"relevancy\": evaluator_r,\n    \"semantic_similarity\": evaluator_s,\n}\nbatch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_results = await batch_runner.aevaluate_responses(\n    queries=eval_qs[:max_samples],\n    responses=pred_responses[:max_samples],\n    reference=ref_response_strs[:max_samples],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_eval_results = await batch_runner.aevaluate_responses(\n    queries=eval_qs[:max_samples],\n    responses=base_pred_responses[:max_samples],\n    reference=ref_response_strs[:max_samples],\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = get_results_df(\n    [eval_results, base_eval_results],\n    [\"Sentence Window Retriever\", \"Base Retriever\"],\n    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n)\ndisplay(results_df)","metadata":{},"execution_count":null,"outputs":[]}]}